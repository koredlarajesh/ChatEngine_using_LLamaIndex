{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Query Engine"
      ],
      "metadata": {
        "id": "__d1MQyvFvIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# install llma-index library"
      ],
      "metadata": {
        "id": "eFpHIeYAFETP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCTkZTavAx8j",
        "outputId": "3fa6a527-7a87-4724-fbc6-fc04d3ad6d04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama_index in /usr/local/lib/python3.10/dist-packages (0.9.29)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama_index) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama_index) (3.9.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from llama_index) (4.12.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.6.3)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.2.14)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.26.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.5.8)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (3.2.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama_index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.23.5)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (4.9.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.9.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama_index) (2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama_index) (1.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index) (4.66.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama_index) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama_index) (1.7.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama_index) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama_index) (1.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama_index) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama_index) (1.0.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama_index) (3.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama_index) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama_index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama_index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama_index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama_index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama_index) (3.20.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index) (2023.3.post1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama_index) (1.2.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama_index) (23.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama_index) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama_index"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import Open AI Key"
      ],
      "metadata": {
        "id": "jl6jqMt_FMPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\""
      ],
      "metadata": {
        "id": "Afg-L0UGA3SA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# create directory then load documents"
      ],
      "metadata": {
        "id": "snSa_oUqFTLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "# you can use wget down load required documents here to data folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6b6kozoBUZ_",
        "outputId": "e5693fc5-ad7c-492f-e95f-41dfc9163be6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from llama_index import download_loader"
      ],
      "metadata": {
        "id": "c2fcKrvIBaur"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pdf reader for loading document\n",
        "PDFReader = download_loader(\"PDFReader\")\n",
        "loader=PDFReader()\n",
        "#documents=loader.load_data(file=Path(\"./data/11230024851101_POLICY_DOC.pdf\"))\n",
        "documents=loader.load_data(file=Path(\"./data/TextEmbeddingswithLLM.pdf\"))"
      ],
      "metadata": {
        "id": "YDRbGc79B1v7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKcYX90iCM4g",
        "outputId": "4d044de4-7c14-4062-c84e-c3672194cbfb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# indexing"
      ],
      "metadata": {
        "id": "qO1ja6KzGHgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex\n",
        "index = VectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "KpGSpxdEDAYG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuring Retriever"
      ],
      "metadata": {
        "id": "9ofyJfh3GMqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.retrievers import VectorIndexRetriever\n",
        "retriever=VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=10,\n",
        ")"
      ],
      "metadata": {
        "id": "xhkpx3l-DPKJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuring Response Synthesizer"
      ],
      "metadata": {
        "id": "IF2Kv71dGR9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.response_synthesizers import get_response_synthesizer\n",
        "response_synthesizer = get_response_synthesizer(\n",
        "    response_mode=\"compact\"\n",
        ")"
      ],
      "metadata": {
        "id": "IzWFHRw_DwdG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Engine"
      ],
      "metadata": {
        "id": "zNeDr6LDGaAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine=index.as_query_engine(\n",
        "    retriever=retriever,\n",
        "    response_synthesizer=response_synthesizer\n",
        ")"
      ],
      "metadata": {
        "id": "KAclEMXDEJc4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"Give me who are the authors for this document\"\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgxlAAn2G0rP",
        "outputId": "3c274407-da79-4d69-a6b6-6d2f531711c2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The authors for this document are Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, and others.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.source_nodes[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "zUTTrss6HNma",
        "outputId": "e9f8270c-6948-4890-e1c6-fc7d1bd59cbb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https:\\n//aclanthology.org/N18-1074 .\\n[42] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\\nfoundation and fine-tuned chat models. ArXiv preprint , abs/2307.09288, 2023. URL https:\\n//arxiv.org/abs/2307.09288 .\\n[43] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. GPL: Generative pseudo\\nlabeling for unsupervised domain adaptation of dense retrieval. In Proceedings of the 2022\\nConference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies , pages 2345–2360, Seattle, United States, 2022. Association\\nfor Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.168. URL https://\\naclanthology.org/2022.naacl-main.168 .\\n[44] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan\\nMajumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training.\\nArXiv preprint , abs/2212.03533, 2022. URL https://arxiv.org/abs/2212.03533 .\\n[45] Liang Wang, Nan Yang, and Furu Wei. Query2doc: Query expansion with large language\\nmodels. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language\\nProcessing , pages 9414–9423, Singapore, December 2023. Association for Computational\\nLinguistics. doi: 10.18653/v1/2023.emnlp-main.585. URL https://aclanthology.org/\\n2023.emnlp-main.585 .\\n12'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"Give me title of document\"\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_W818voFEdsH",
        "outputId": "339067d2-fd50-45c7-f0b4-7f5b621b8902"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextEmbeddingswithLLM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"Give me what document main objective\"\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bz9_M2KEEqeU",
        "outputId": "737dca5f-e861-43c3-d9e0-0bb5336d1fd1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The main objective of the document is to present a novel approach to train state-of-the-art text embeddings by exploiting the latest advances of Language Model Models (LLMs) and synthetic data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"Give me what are the llm models author speaking in this document\"\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaQytEMbE6v0",
        "outputId": "7bd667a7-ab69-4eb6-cd3d-55144be98533"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The llm models mentioned in this document are LLaMA-2, Mistral, RepLLaMA, SGPT, GTR, and Udever.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"Give me what is the llama-2\"\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJxfy-a4Gtde",
        "outputId": "7e505418-025c-4772-f2fa-bdffbb0f5d18"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Llama-2 is an open foundation and fine-tuned chat model. It is a notable effort in bridging the gap between proprietary and open-source Language Model (LLM) systems. Llama-2 aims to enhance the capabilities of LLMs by incorporating external information and improving text embeddings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(response.source_nodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHE5R3ElHIGs",
        "outputId": "bb553ce6-8e15-4c80-e341-51a49d4a1bc9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"summarize document using 1000 words \"\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEAweUZyHkDJ",
        "outputId": "ad67a259-a3e9-4587-914d-678961483cf0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The document provides instructions for various tasks and datasets used for evaluation. It also mentions the performance of different models on these tasks. However, there is no specific information about summarizing a document using 1000 words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this is the challenge with query engine it wont track conversation happend in the past\n",
        "\n",
        "## **for solving that ChatEngine came to picture **"
      ],
      "metadata": {
        "id": "rAZ0kiWVIR7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Engine"
      ],
      "metadata": {
        "id": "zIAiPYEoIzI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_engine=index.as_chat_engine(retrievr=retriever,\n",
        "                                 response_synthesizer=response_synthesizer)"
      ],
      "metadata": {
        "id": "EHztWzP7H7ll"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=chat_engine.chat(\n",
        "    \"what are the llm models author spoke in this document\"\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoCKnMLIJG9n",
        "outputId": "a40be7f8-0753-47b4-f555-e2d0aed6f1eb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The author mentioned two LLM models in the document: LLaMA-2 and Mistral.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=chat_engine.chat(\n",
        "    \"what are the llm models author spoke in this document, i want all llm models, u gave response is Based on the document, the author mentioned two LLM models: LLaMA-2 and Mistral. but what are all these  RepLLaMA, SGPT, GTR, and Udever. \"\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok88J10SJXLA",
        "outputId": "8b0b1ba2-b830-4eee-efd2-14702c5d0b31"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In addition to LLaMA-2 and Mistral, the author also mentioned other LLM models in the document. These include:\n",
            "\n",
            "1. GPT-4: GPT-4 is a highly advanced LLM model, but it is proprietary and has limited technical details disclosed.\n",
            "\n",
            "2. GPL: GPL is another LLM model, but specific details about it are not provided in the document.\n",
            "\n",
            "3. ChatGPT: ChatGPT is an LLM model that excels in conversational tasks and natural language understanding.\n",
            "\n",
            "4. RepLLaMA: RepLLaMA is a model that enhances text embeddings based on LLMs.\n",
            "\n",
            "5. SGPT: SGPT is another model that builds upon LLMs to improve text embeddings.\n",
            "\n",
            "6. GTR: GTR is an LLM model that focuses on generating text responses.\n",
            "\n",
            "7. Udever: Udever is an LLM model, but no specific details about it are mentioned in the document.\n",
            "\n",
            "These are the LLM models mentioned in the document.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat_engine.chat(\n",
        "    \"summarize document using 1000 words \"\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu65WTGYJwRi",
        "outputId": "33b90491-3d80-42dd-ded0-8ff59344c206"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The document discusses experiments conducted using synthetic data and the fine-tuning and evaluation of a model. The model achieved the highest average score on the MTEB benchmark, surpassing the previous state-of-the-art model by 2.4 points. The experiments involved using generated synthetic data and a collection of public datasets for training the model. The evaluation of the model on the MTEB benchmark took approximately 3 days using 8V100 GPUs. Notably, the model's performance remained competitive even when trained solely on synthetic data without any labeled data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T5YXRFiPKYwf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}